{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "### Notes from implementation\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. \n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Tensorflow Version: 1.4.0   Keras Version : 2.1.3 \n",
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FCN_INPUT_SHAPE                [128 128]\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                7\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import  gc\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pprint\n",
    "import keras.backend as KB\n",
    "sys.path.append('../')\n",
    "\n",
    "import mrcnn.model     as modellib\n",
    "import mrcnn.visualize as visualize\n",
    "import mrcnn.shapes    as shapes\n",
    "from mrcnn.config      import Config\n",
    "from mrcnn.model       import log\n",
    "from mrcnn.dataset     import Dataset \n",
    "# from mrcnn.pc_layer    import PCTensor\n",
    "from mrcnn.pc_layer   import PCNLayer\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "MODEL_PATH = 'E:\\Models'\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(MODEL_PATH, \"mrcnn_logs\")\n",
    "# Path to COCO trained weights\n",
    "COCO_MODEL_PATH   = os.path.join(MODEL_PATH, \"mask_rcnn_coco.h5\")\n",
    "RESNET_MODEL_PATH = os.path.join(MODEL_PATH, \"resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "\n",
    "print(\"Tensorflow Version: {}   Keras Version : {} \".format(tf.__version__,keras.__version__))\n",
    "pp = pprint.PrettyPrinter(indent=2, width=100)\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "# Build configuration object -----------------------------------------------\n",
    "config = shapes.ShapesConfig()\n",
    "config.BATCH_SIZE      = 2                    #Batch size is 2 (# GPUs * images/GPU).\n",
    "config.IMAGES_PER_GPU  = 2\n",
    "config.STEPS_PER_EPOCH = 7\n",
    "# config.IMAGES_PER_GPU  = 1\n",
    "config.FCN_INPUT_SHAPE = config.IMAGE_SHAPE[0:2]\n",
    "config.display() \n",
    "\n",
    "# Build shape dataset        -----------------------------------------------\n",
    "\n",
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "\n",
    "# Training dataset\n",
    "# generate 500 shapes \n",
    "dataset_train = shapes.ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = shapes.ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()\n",
    "\n",
    "# Load and display random samples\n",
    "# image_ids = np.random.choice(dataset_train.image_ids, 3)\n",
    "# for image_id in [3]:\n",
    "#     image = dataset_train.load_image(image_id)\n",
    "#     mask, class_ids = dataset_train.load_mask(image_id)\n",
    "#     visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " COCO Model Path       :  E:\\Models\\mask_rcnn_coco.h5\n",
      " Checkpoint folder Path:  E:\\Models\\mrcnn_logs\n",
      ">>> Set_log_dir() -- model dir is  E:\\Models\\mrcnn_logs\n",
      "    set_log_dir: Checkpoint path set to : E:\\Models\\mrcnn_logs\\shapes20180403T1937\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      ">>> generate_anchors()\n",
      " meshgrid scales and ratios:  (3, 1) (3, 1)\n",
      " flattened meshgrid scales and ratios:  (3,) (3,)\n",
      " Heights  [11.3137085   8.          5.65685425]  widths   [ 5.65685425  8.         11.3137085 ]\n",
      " Strides shift_x, shift_y:\n",
      "  [  0   4   8  12  16  20  24  28  32  36  40  44  48  52  56  60  64  68  72  76  80  84  88  92\n",
      "  96 100 104 108 112 116 120 124] \n",
      " [  0   4   8  12  16  20  24  28  32  36  40  44  48  52  56  60  64  68  72  76  80  84  88  92\n",
      "  96 100 104 108 112 116 120 124]\n",
      " meshgrid shift_x, shift_y:  (32, 32) (32, 32)\n",
      " box_widths   (1024, 3)  box_cneterss:  (1024, 3)\n",
      " box_heights  (1024, 3)  box_cneters_y:  (1024, 3)\n",
      " box_centers stack   : (1024, 3, 2)\n",
      " box_centers reshape : (3072, 2)\n",
      " box_sizes   stack   : (1024, 3, 2)\n",
      " box_sizes   reshape : (3072, 2)\n",
      " Anchor boxes shape is :  (3072, 4)\n",
      ">>> generate_anchors()\n",
      " meshgrid scales and ratios:  (3, 1) (3, 1)\n",
      " flattened meshgrid scales and ratios:  (3,) (3,)\n",
      " Heights  [22.627417  16.        11.3137085]  widths   [11.3137085 16.        22.627417 ]\n",
      " Strides shift_x, shift_y:\n",
      "  [  0   8  16  24  32  40  48  56  64  72  80  88  96 104 112 120] \n",
      " [  0   8  16  24  32  40  48  56  64  72  80  88  96 104 112 120]\n",
      " meshgrid shift_x, shift_y:  (16, 16) (16, 16)\n",
      " box_widths   (256, 3)  box_cneterss:  (256, 3)\n",
      " box_heights  (256, 3)  box_cneters_y:  (256, 3)\n",
      " box_centers stack   : (256, 3, 2)\n",
      " box_centers reshape : (768, 2)\n",
      " box_sizes   stack   : (256, 3, 2)\n",
      " box_sizes   reshape : (768, 2)\n",
      " Anchor boxes shape is :  (768, 4)\n",
      ">>> generate_anchors()\n",
      " meshgrid scales and ratios:  (3, 1) (3, 1)\n",
      " flattened meshgrid scales and ratios:  (3,) (3,)\n",
      " Heights  [45.254834 32.       22.627417]  widths   [22.627417 32.       45.254834]\n",
      " Strides shift_x, shift_y:\n",
      "  [  0  16  32  48  64  80  96 112] \n",
      " [  0  16  32  48  64  80  96 112]\n",
      " meshgrid shift_x, shift_y:  (8, 8) (8, 8)\n",
      " box_widths   (64, 3)  box_cneterss:  (64, 3)\n",
      " box_heights  (64, 3)  box_cneters_y:  (64, 3)\n",
      " box_centers stack   : (64, 3, 2)\n",
      " box_centers reshape : (192, 2)\n",
      " box_sizes   stack   : (64, 3, 2)\n",
      " box_sizes   reshape : (192, 2)\n",
      " Anchor boxes shape is :  (192, 4)\n",
      ">>> generate_anchors()\n",
      " meshgrid scales and ratios:  (3, 1) (3, 1)\n",
      " flattened meshgrid scales and ratios:  (3,) (3,)\n",
      " Heights  [90.50966799 64.         45.254834  ]  widths   [45.254834   64.         90.50966799]\n",
      " Strides shift_x, shift_y:\n",
      "  [ 0 32 64 96] \n",
      " [ 0 32 64 96]\n",
      " meshgrid shift_x, shift_y:  (4, 4) (4, 4)\n",
      " box_widths   (16, 3)  box_cneterss:  (16, 3)\n",
      " box_heights  (16, 3)  box_cneters_y:  (16, 3)\n",
      " box_centers stack   : (16, 3, 2)\n",
      " box_centers reshape : (48, 2)\n",
      " box_sizes   stack   : (16, 3, 2)\n",
      " box_sizes   reshape : (48, 2)\n",
      " Anchor boxes shape is :  (48, 4)\n",
      ">>> generate_anchors()\n",
      " meshgrid scales and ratios:  (3, 1) (3, 1)\n",
      " flattened meshgrid scales and ratios:  (3,) (3,)\n",
      " Heights  [181.01933598 128.          90.50966799]  widths   [ 90.50966799 128.         181.01933598]\n",
      " Strides shift_x, shift_y:\n",
      "  [ 0 64] \n",
      " [ 0 64]\n",
      " meshgrid shift_x, shift_y:  (2, 2) (2, 2)\n",
      " box_widths   (4, 3)  box_cneterss:  (4, 3)\n",
      " box_heights  (4, 3)  box_cneters_y:  (4, 3)\n",
      " box_centers stack   : (4, 3, 2)\n",
      " box_centers reshape : (12, 2)\n",
      " box_sizes   stack   : (4, 3, 2)\n",
      " box_sizes   reshape : (12, 2)\n",
      " Anchor boxes shape is :  (12, 4)\n",
      " soize of anchor arrya is : (4092, 4)\n",
      "<class 'list'>\n",
      "Tensor(\"rpn_class_logits_12/concat:0\", shape=(?, ?, 2), dtype=float32) rpn_class_logits_12/concat:0\n",
      "Tensor(\"rpn_class_12/concat:0\", shape=(?, ?, 2), dtype=float32) rpn_class_12/concat:0\n",
      "Tensor(\"rpn_bbox_12/concat:0\", shape=(?, ?, 4), dtype=float32) rpn_bbox_12/concat:0\n",
      "Proposal Layer init complete. Size of anchors:  (4092, 4)\n",
      ">>> Detection Target Layer : initialization\n",
      ">>> Detection Target Layer : call \n",
      ">>> Detection Target Layer : return call  <class 'list'>\n",
      ">>> PCN Layer : initialization\n",
      ">>> PCN Layer : call\n",
      " shape of pcn_gaussian is  <unknown>\n",
      ">>> FCN Layer \n",
      " height : 128 width : 128 depth : 4\n",
      " feature map shape is  (?, 128, 128, 4)\n",
      " rpn_bbox_loss_graph\n",
      "rpn_match size : (?, ?)\n",
      "rpn_bbox  size : (?, ?, 4)\n",
      " tf default session:  None\n",
      " rpn_bbox_loss_graph\n",
      "rpn_match size : (?, ?)\n",
      "rpn_bbox  size : (?, ?, 4)\n",
      " tf default session:  None\n",
      ">>> CLSLoss Layer : initialization\n",
      ">>> CLS Loss Layer : call\n",
      "    target_class_ids   .shape/type  : (2, ?)\n",
      "    mrcnn_class_logits .shape/type  : (?, 32, 4)\n",
      "    activate_class_ids .shape/type  : (?, ?)\n",
      ">>> MaskRCNN build complete\n",
      ">>> MaskRCNN initialization complete\n",
      ">>> find_last checkpoint file() \n",
      "    find_last info:   dir_name: E:\\Models\\mrcnn_logs\\shapes20180313T1856\n",
      "    find_last info: checkpoint: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0242.h5\n",
      ">>> load_weights()\n",
      "    load_weights: Loading weights from: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0242.h5\n"
     ]
    }
   ],
   "source": [
    "print(' COCO Model Path       : ', COCO_MODEL_PATH)\n",
    "print(' Checkpoint folder Path: ', MODEL_DIR)\n",
    "# del history\n",
    "try :\n",
    "    del model\n",
    "    gc.collect()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "#model.keras_model.summary(line_length = 120)\n",
    "# print(model.find_last())\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"last\"  # imagenet, coco, or last\n",
    "if init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    loc=model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    loc= model.load_weights(model.find_last()[1], by_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training head using  Keras.model.fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs_to_run =2, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Training heads using train_in_batches ()\n",
    "\n",
    "We need to use this method for the time being as the fit generator does not have provide EASY access to the output in Keras call backs. By training in batches, we pass a batch through the network, pick up the generated RoI detections and bounding boxes and generate our semantic / gaussian tensors ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_in_batches(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs_to_run = 2,\n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate one training iteration - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "np.set_printoptions(linewidth=100)\n",
    "learning_rate=model.config.LEARNING_RATE\n",
    "epochs_to_run = 2\n",
    "layers='heads'\n",
    "batch_size = 0\n",
    "steps_per_epoch = 0\n",
    "# assert self.mode == \"training\", \"Create model in training mode.\"\n",
    "# Pre-defined layer regular expressions\n",
    "layer_regex = {\n",
    "    # all layers but the backbone\n",
    "    \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # From a specific Resnet stage and up\n",
    "    \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # All layers\n",
    "    \"all\": \".*\",\n",
    "}\n",
    "\n",
    "if layers in layer_regex.keys():\n",
    "    layers = layer_regex[layers]\n",
    "if batch_size == 0 :\n",
    "    batch_size = model.config.BATCH_SIZE            \n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = model.config.STEPS_PER_EPOCH\n",
    "\n",
    "# Data generators\n",
    "train_generator = data_generator(dataset_train, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size)\n",
    "val_generator   = data_generator(dataset_val, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 augment=False)\n",
    "\n",
    "# Train\n",
    "log(\"Last epoch completed : {} \".format(model.epoch))\n",
    "log(\"Starting from epoch {} for {} epochs. LR={}\".format(model.epoch, epochs_to_run, learning_rate))\n",
    "log(\"Steps per epoch:    {} \".format(steps_per_epoch))\n",
    "log(\"Batchsize      :    {} \".format(batch_size))\n",
    "log(\"Checkpoint Folder:  {} \".format(model.checkpoint_path))\n",
    "epochs = model.epoch + epochs_to_run\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "if not gfile.IsDirectory(model.log_dir):\n",
    "    log('Creating checkpoint folder')\n",
    "    gfile.MakeDirs(model.log_dir)\n",
    "else:\n",
    "    log('Checkpoint folder already exists')\n",
    "\n",
    "model.set_trainable(layers)            \n",
    "model.compile(learning_rate, model.config.LEARNING_MOMENTUM)        \n",
    "\n",
    "out_labels = model.keras_model._get_deduped_metrics_names()\n",
    "callback_metrics = out_labels + ['val_' + n for n in out_labels]\n",
    "\n",
    "progbar = keras.callbacks.ProgbarLogger(count_mode='steps')\n",
    "progbar.set_model(model.keras_model)\n",
    "progbar.set_params({\n",
    "    'epochs': epochs,\n",
    "    'steps': steps_per_epoch,\n",
    "    'verbose': 1,\n",
    "    'do_validation': False,\n",
    "    'metrics': callback_metrics,\n",
    "})\n",
    "\n",
    "progbar.set_model(model.keras_model) \n",
    "\n",
    "chkpoint = keras.callbacks.ModelCheckpoint(model.checkpoint_path, \n",
    "                                           monitor='loss', verbose=1, save_best_only = True, save_weights_only=True)\n",
    "chkpoint.set_model(model.keras_model)\n",
    "\n",
    "progbar.on_train_begin()\n",
    "epoch_idx = model.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate one training iteration - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch_idx >= epochs:\n",
    "    print('Final epoch {} has already completed - Training will not proceed'.format(epochs))\n",
    "\n",
    "# while epoch_idx < epochs :\n",
    "progbar.on_epoch_begin(epoch_idx)\n",
    "steps_index = 0\n",
    "# for steps_index in range(steps_per_epoch):\n",
    "\n",
    "batch_logs = {}\n",
    "print(' self.epoch {}   epochs {}  step {} '.format(model.epoch, epochs, steps_index))\n",
    "batch_logs['batch'] = steps_index\n",
    "batch_logs['size']  = batch_size\n",
    "progbar.on_batch_begin(steps_index, batch_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate one training iteration - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgmeta_idx= model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta  =  train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "outs = model.keras_model.train_on_batch(train_batch_x, train_batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process outside of training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(' COCO Model Path       : ', COCO_MODEL_PATH)\n",
    "print(' Checkpoint folder Path: ', MODEL_DIR)\n",
    "# del history\n",
    "try :\n",
    "    del model\n",
    "    gc.collect()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "#model.keras_model.summary(line_length = 120) \n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"last\"  # imagenet, coco, or last\n",
    "if init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    loc=model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    loc= model.load_weights(model.find_last()[1], by_name=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator = data_generator(dataset_train, model.config, shuffle=True,\n",
    "                                 batch_size=model.config.BATCH_SIZE,\n",
    "                                 augment = False)\n",
    "val_generator = data_generator(dataset_val, model.config, shuffle=True, \n",
    "                                batch_size=model.config.BATCH_SIZE,\n",
    "                                augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile_only(learning_rate=config.LEARNING_RATE, layers='heads')\n",
    "# print(KB.eval(KB.learning_phase()))\n",
    "KB.set_learning_phase(1)\n",
    "print(' Learning phase values is L ' ,KB.learning_phase())\n",
    "mm = model.keras_model\n",
    "print('\\n Metrics (_get_deduped_metrics_names():) ') \n",
    "pp.pprint(mm._get_deduped_metrics_names())\n",
    "print('\\n Outputs: ') \n",
    "pp.pprint(mm.outputs)\n",
    "print('\\n Losses (model.metrics_names): ') \n",
    "# pp.pprint(mm.losses)\n",
    "pp.pprint(mm.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get next shapes from generator and display loaded shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display loaded shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_batch_x, train_batch_y = next(train_generator)\n",
    "imgmeta_idx = model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   mrcnn.utils            import parse_image_meta_graph\n",
    "a,b,c,d = parse_image_meta_graph(img_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Data thru model using get_layer_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mrcnn.callbacks import get_layer_output_1,get_layer_output_2\n",
    "np.set_printoptions(linewidth=100,precision=4)\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    " \n",
    "layers_out = get_layer_output_2(model.keras_model, train_batch_x, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layers_out[16], layers_out[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgmeta_idx = mm.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot mask in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=99999, linewidth=2000)\n",
    "# print(np.array2string(mask[...,0],max_line_width=2000,separator=''))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outmask0 = layers_out[14][0,:,:,:,1] ##  mrcnn_mask\n",
    "np.max(outmask0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Predicition Probability Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mrcnn.visualize import plot_gaussian\n",
    "Zout = layers_out[1]\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout[img,cls], title = ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Ground Truth Probability tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# print(layers_out[i].shape)      #[0,0,0:20, 0:20]\n",
    "Zout = layers_out[2]\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout[img,cls], title = ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display predicted bounding boxes - calculate center and width/height of bboxes displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.utils import trim_zeros\n",
    "np.set_printoptions( edgeitems=32, suppress=True)\n",
    "pred_bb = layers_out[3]\n",
    "print(pred_bb.shape)\n",
    "x0 = [ trim_zeros((pred_bb[0,i,:,:])) for i in range(4)]\n",
    "ps0 = np.concatenate( x0, axis=0 )\n",
    "\n",
    "x1 = [ trim_zeros((pred_bb[1,i,:,:])) for i in range(4)]\n",
    "ps1 = np.concatenate( x1, axis=0 )\n",
    "# print(np.concatenate( x1, axis=0 ))\n",
    "print(ps0)\n",
    "print(ps0.shape)\n",
    "width  = ps0[:,5] - ps0[:,3]\n",
    "height = ps0[:,4] - ps0[:,2]\n",
    "cx     = ps0[:,3] + ( width  / 2.0)\n",
    "cy     = ps0[:,2] + ( height / 2.0)\n",
    "means0  = np.stack((cx,cy,width, height),axis = -1)\n",
    "print(means0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output RoIs (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rois = layers_out[0]\n",
    "output_rois[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from mrcnn.pc_layer import PCTensor\n",
    "# np.set_printoptions(precision=4,edgeitems=32)\n",
    "# pc_tensor = PCTensor(model)\n",
    "# pc_tensor.build_predictions(train_batch_x)\n",
    "# # pc_tensor.pred_tensor[1]\n",
    "# pc_tensor.pred_stacked[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display ground truth bboxes from Shapes database (using load_image_gt)\n",
    "\n",
    "Here we are displaying the ground truth bounding boxes as provided by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "p_original_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "# print(p_gt_class_id.shape, p_gt_bbox.shape, p_gt_mask.shape)\n",
    "print(p_gt_bbox)\n",
    "visualize.draw_boxes(p_original_image, p_gt_bbox)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "p_original_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "# print(p_gt_class_id.shape, p_gt_bbox.shape, p_gt_mask.shape)\n",
    "print(p_gt_bbox)\n",
    "visualize.draw_boxes(p_original_image, p_gt_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display bboxes from Ground Truth Info - Input info Passed to Network \n",
    "\n",
    "layers_out[5]  gt_tensor is based on input_gt_class_ids and input_normlzd_gt_boxes\n",
    "\n",
    "Display the Ground Truth bounding boxes from the tensor we've constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=120, precision=5)\n",
    "gt_bboxes = layers_out[5]  \n",
    "print(layers_out[5].shape)\n",
    "print(' gt_cls_cnt')\n",
    "print(layers_out[6])\n",
    "print(layers_out[5][1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gt_bboxes)\n",
    "# visualize.display_instances(p_original_image, p_gt_bbox, p_gt_mask, p_gt_class_id, \n",
    "#                             dataset_train.class_names, figsize=(8, 8))\n",
    "# pp.pprint(gt_bboxes)\n",
    "img = 0\n",
    "image_id = img_meta[img,0]\n",
    "print('Image id: ',image_id)\n",
    "p_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "gt_bboxes = layers_out[5]\n",
    "print(gt_bboxes.shape)\n",
    "print(gt_bboxes[0,1,0:1,2:6])\n",
    "print(gt_bboxes[0,2,0:2,2:6])\n",
    "gt_bb = np.vstack((gt_bboxes[0,1,0:1,2:6],gt_bboxes[0,2,0:2,2:6],gt_bboxes[0,3,0:2,2:6]))\n",
    "gt_bb.shape\n",
    "visualize.draw_boxes(p_image, gt_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display RoI proposals generated\n",
    "\n",
    "Display bounding boxes from tensor of proposals produced by the network \n",
    "Square: 1 , Circle:2 , Triangle -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = 0\n",
    "cls = 3  # <==== Class to dispaly\n",
    "image_id = img_meta[img,0]\n",
    "print('Image id: ',image_id)\n",
    "p_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "\n",
    "pred_tensor = layers_out[3]\n",
    "caps = [str(cls)+'-'+str(x) for x in pred_tensor[img,cls,:,0].astype('int16').tolist() ]\n",
    "print(caps)\n",
    "# print(pc_tensor.pred_tensor[1,3,:])\n",
    "# print(pc_tensor.pred_tensor[1,3,:,2:6])\n",
    "visualize.draw_boxes(p_image, pred_tensor[img,cls,:,2:6], captions = caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class:\n",
    "- determine the center of each bounding box.\n",
    "- center a 2d gaussian distribution with the mean = center of bounding box and sigma = height/width\n",
    "- place dist on mesh grid\n",
    "- normalize\n",
    "- draw heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "from mrcnn.pc_layer import PCTensor\n",
    "pc_tensor = PCTensor(model)\n",
    "pc_tensor.build_predictions(sample_x)\n",
    "print(pc_tensor.pred_stacked)    # list of tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Image 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.visualize import plot_gaussian\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout1[img,cls], title = ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Image 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img = 0\n",
    "# cls = 0\n",
    "# _cnt = pc_tensor.pred_cls_cnt[img,cls]\n",
    "# print(_cnt)\n",
    "# for box in range(_cnt):\n",
    "\n",
    "#     mns = means[img,cls, 0 : _cnt]\n",
    "#     print('img: ',img, 'class: ', cls, 'class count: ',_cnt, 'shape of mns :',mns.shape)\n",
    "#     # print('** bbox is : ' ,self.pred_tensor[img,cls,box])\n",
    "#     # print('    center is ({:4f},{:4f})  width is {:4f} height is {:4f} '\\\n",
    "#         # .format(mns[0],mns[1],width[img,cls,box],height[img,cls,box]))            \n",
    "#     # fn = lambda x: multivariate_normal(x, [[12,0.0] , [0.0,19]])\n",
    "#     # rv = tf.map_fn(fn, \n",
    "#     rv = np.apply_along_axis(multivariate_normal, 1, mns, [[12,0.0] , [0.0,19]])\n",
    "#     print('rv :',rv.shape, rv)\n",
    "#     _zo = rv.pdf(pos[img,cls])\n",
    "#     print('zo :',_zo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tf.contrib.distributions\n",
    "k_sess = KB.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp1 = tf.fill([1,1,32], 12.0)\n",
    "# pp2 = tf.fill([1,1,32], 19.0)\n",
    "# pp  = tf.cast(tf.stack((pp1,pp2),axis=-1), dtype=tf.float64)\n",
    "# tf.cast([12.0, 19.00], dtype=tf.float64)\n",
    "# pp1.eval(session = k_sess)\n",
    "\n",
    "# mvn = tfd.MultivariateNormalDiag(means[0,0,0,:],scale_diag=p1)\n",
    "# mvn = tfd.MultivariateNormalDiag(means[0,0,0,:],scale_diag=p1)\n",
    "\n",
    "# with k_sess.as_default():\n",
    "#     print(mvn.mean())\n",
    "#     print(mvn.batch_shape)\n",
    "#     print(mvn.event_shape)\n",
    "#     print(pos[0,0,:,0,0,:].shape)\n",
    "#     rr = mvn.prob(pos[0,0,:,0,0,:])\n",
    "#     print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# from mrcnn.visualize import plot_gaussian\n",
    "# for i in range(0,config.IMAGES_PER_GPU):\n",
    "#     for j in range(0,config.NUM_CLASSES):\n",
    "#         ttl = 'image : {} class: {}'.format(i,j)\n",
    "#         plot_gaussian(Zout[i,j] , title = ttl )\n",
    "# # plot_gaussian(Zout[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zout = np.zeros((num_classes, 128,128))\n",
    "for i in range(1,config.NUM_CLASSES):\n",
    "    print('class: ',i)\n",
    "    for j in range(gt_cls_cnt[i]):\n",
    "        Zout[i] = bbox_gaussian(gt_cpb[i,j], Zout[i])\n",
    "print(Zout.shape)\n",
    " \n",
    "# plot_gaussian(Zout[1])\n",
    "# plot_gaussian(Zout[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "Fine tune all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=211,\n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_bbox)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lay = mm.layers[229]\n",
    "# print(lay.__class__, lay.__class__.__name__)\n",
    "# pp.pprint(dir(lay))\n",
    "# pp.pprint(lay.input_spec.__dict__)\n",
    "# pp.pprint(lay.output.__dict__)\n",
    "# print(type(lay.output))\n",
    "# print(keras.backend.is_keras_tensor(lay))\n",
    "# print(K.eval(lay.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred_index.shape, pred_class.shape, pred_prob.shape)\n",
    "# b_cpb = np.column_stack((pred_index, pred_class, pred_prob, rois)) # , b_probs)) #.transpose()\n",
    "# print(' b_cpb shape: ',b_cpb.shape,'\\n',b_cpb)\n",
    "\n",
    "# print(b_cpb[:,3:] bbox_delta)\n",
    "\n",
    "# nonbg_idx = np.argwhere(b_cpb[:,1]) \n",
    "\n",
    "# print(type(nonbg_idx))\n",
    "# b_cpb_nonbg = b_cpb[nonbg_idx,:].squeeze()\n",
    "\n",
    "# print(b_cpb_nonbg)\n",
    "# order = b_cpb_nonbg[:,2].argsort()\n",
    "\n",
    "\n",
    "\n",
    "# print('\\n srtd_cpb : (idx, class, prob, y1, x1, y2, x2)',srtd_cpb.shape, '\\n')\n",
    "# print(srtd_cpb)\n",
    "\n",
    "# # srtd_cpb_2 has (idx, cls_idx, prob, cx ,cy, width, height) instead of (idx, cls_idx, prob, y1, x1, y2, x2)\n",
    "\n",
    "# width  = srtd_cpb[:,6]-srtd_cpb[:,4]\n",
    "# height = srtd_cpb[:,5]-srtd_cpb[:,3]\n",
    "# cx = srtd_cpb[:,4] + ( width  / 2.0)\n",
    "# cy = srtd_cpb[:,3] + ( height / 2.0)\n",
    "# print('\\n srtd_cpb_2 : (idx, class, prob, cx ,cy, width, height) instead of (y1, x1, y2, x2)')\n",
    "# srtd_cpb_2 = np.column_stack((srtd_cpb[:, 0:3], cx,cy, width, height ))\n",
    "\n",
    "# print('\\n',srtd_cpb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_layer_output(model, model_input,output_layer, training_flag = True):\n",
    "#     _my_input = model_input \n",
    "#     for name,inp in zip(model.input_names, model_input):\n",
    "#         print(' Input Name:  ({:24}) \\t  Input shape: {}'.format(name, inp.shape))\n",
    "\n",
    "\n",
    "#     _mrcnn_class = KB.function(model.input , model.output)\n",
    "#     output = _mrcnn_class(_my_input)                  \n",
    "    \n",
    "#     for name,out in zip (model.output_names,output):\n",
    "#         print(' Output Name: ({:24}) \\t Output shape: {}'.format(name, out.shape))\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_tensor(model):\n",
    "    pred_cpb_all = np.empty((0,8))\n",
    "    for i in range(1,model.config.NUM_CLASSES):\n",
    "    if pred_cls_cnt[i] > 0:\n",
    "        pred_cpb_all = np.vstack((pred_cpb_all, pred_cpb[i,0:pred_cls_cnt[i]] ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import  multivariate_normal\n",
    "import numpy as np\n",
    "def bbox_gaussian( bbox, Zin ):\n",
    "    \"\"\"\n",
    "    receive a bounding box, and generate a gaussian distribution centered on the bounding box and with a \n",
    "    covariance matrix based on the width and height of the bounding box/. \n",
    "    Inputs : \n",
    "    --------\n",
    "    bbox :  (index, class_id, class_prob, y1, x1, y2, x2)\n",
    "    bbox :  (index, class_id, class_prob, cx, cy, width, height)\n",
    "    Returns:\n",
    "    --------\n",
    "    bbox_g  grid mesh [image_height, image width] covering the distribution\n",
    "\n",
    "    \"\"\"\n",
    "    print(bbox.shape)\n",
    "    width  = bbox[6] - bbox[4]\n",
    "    height = bbox[5] - bbox[3]\n",
    "    cx     = bbox[4] + ( width  / 2.0)\n",
    "    cy     = bbox[3] + ( height / 2.0)\n",
    "#     cx, cy, width, height = bbox[3:]\n",
    "    print('center is ({},{}) width: {}  height: {} '.format(cx, cy, width,  height))\n",
    "#     srtd_cpb_2 = np.column_stack((srtd_cpb[:, 0:2], cx,cy, width, height ))\n",
    "    X = np.arange(0, 128, 1)\n",
    "    Y = np.arange(0, 128, 1)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    pos = np.empty(X.shape+(2,))   # concatinate shape of x to make ( x.rows, x.cols, 2)\n",
    "    pos[:,:,0] = X;\n",
    "    pos[:,:,1] = Y;\n",
    "\n",
    "    rv = multivariate_normal([cx,cy],[[12,0.0] , [0.0,19]])\n",
    "    Zout  = rv.pdf(pos)\n",
    "    Zout += Zin\n",
    "    return Zout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as KB\n",
    "# if 'tensorflow' == KB.backend():\n",
    "#     import tensorflow as tf\n",
    "#     from keras.backend.tensorflow_backend import set_session\n",
    "#     # tfconfig = tf.ConfigProto(\n",
    "#         # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5),\n",
    "#         # device_count = {'GPU': 1}\n",
    "#     # )    \n",
    "#     tfconfig = tf.ConfigProto()\n",
    "#     tfconfig.gpu_options.allow_growth=True\n",
    "#     tfconfig.gpu_options.visible_device_list = \"0\"\n",
    "#     tfconfig.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "#     tf_sess = tf.Session(config=tfconfig)\n",
    "#     set_session(tf_sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Prediction Tensor (output tensor)\n",
    "Using the softmax outputs from the mrcnn head of the network and predicted RoIs , \n",
    "build tensor [ num-classes, num_rois, (roi_information) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=4)\n",
    "\n",
    "# // pass model to TensorBuilder\n",
    "\n",
    "# h, w = config.IMAGE_SHAPE[:2]\n",
    "\n",
    "# class_idx = mm.output_names.index('mrcnn_class')\n",
    "# bbox_idx  = mm.output_names.index('mrcnn_bbox')\n",
    "# outroi_idx= mm.output_names.index('output_rois')\n",
    "\n",
    "\n",
    "# print('mrcnn_class idx: {}   mrcnn_bbox idx : {}   output_rois idx : {}'.format(class_idx, bbox_idx,outroi_idx))\n",
    "\n",
    "\n",
    "# mrcnn_class = ppp[class_idx]\n",
    "# mrcnn_bbox  = ppp[bbox_idx]\n",
    "# rois_norm   = ppp[outroi_idx][0,...] \n",
    "# rois        = rois_norm * np.array([h,w,h,w])\n",
    "\n",
    "# num_classes = config.NUM_CLASSES\n",
    "# num_rois    = config.TRAIN_ROIS_PER_IMAGE\n",
    "# num_max_gt  = config.DETECTION_MAX_INSTANCES\n",
    "# num_cols    = 8 \n",
    "\n",
    "# pred_arr    = np.zeros((num_classes, num_rois, num_cols ))      # 4, 32, 7\n",
    "# pred_cpb    = np.zeros_like(pred_cpb)\n",
    "# pred_cls_cnt= np.zeros((num_classes), dtype='int16')\n",
    "\n",
    "# print(' mrcnn_bbox shape is : ',mrcnn_bbox.shape, ' pred_cpb shape is   : ',pred_cpb.shape  )\n",
    "\n",
    "# # use the argmaxof each row to determine the dominating (predicted) class\n",
    "# #---------------------------------------------------------------------------\n",
    "# pred_class = np.argmax(mrcnn_class[0,:,:],axis=1).astype('int16')   # (32,)\n",
    "\n",
    "# # pred_index = np.arange(pred_class.shape[0],dtype='int16')\n",
    "# # pred_prob  =    np.max(mrcnn_class[0,:,:],axis=1)                   #  (32,)\n",
    "# # dont need it for now. Need to see if and how we should apply  the delta to the bounding box coords\n",
    "# # pred_delta   = mrcnn_bbox[0,pred_index[:],pred_class[:],:]        \n",
    "\n",
    "# for i in range(num_classes) :\n",
    "#     class_idxs = np.argwhere(pred_class == i )\n",
    "#     pred_cls_cnt[i] = class_idxs.shape[0] \n",
    "#     for j , c_idx in enumerate(class_idxs):\n",
    "#         pred_arr[i, j,  0]  = j\n",
    "#         pred_arr[i, j,  1]  = i                                   # class_id\n",
    "#         pred_arr[i, j,  2]  = np.max(mrcnn_class[0, c_idx ])      # probability\n",
    "#         pred_arr[i, j,3:7]  = rois[c_idx]                         # roi coordinates\n",
    "#         pred_arr[i, j,  7]  = c_idx                               # index from mrcnn_class array (temp for verification)\n",
    "        \n",
    "        \n",
    "# # sort each class in descending prediction order \n",
    "\n",
    "# order = pred_arr[:,:,2].argsort()\n",
    "\n",
    "# for i in range(num_classes):\n",
    "#     pred_cpb[i,:,1:] =  pred_arr[i,order[i,::-1],1:]      \n",
    "# pred_cpb[:,:,0] = pred_arr[:,:,0]\n",
    "\n",
    "# print('pred_cpb shape', pred_cpb.shape)\n",
    "\n",
    "# print(pred_cpb_all)\n",
    "\n",
    "# Display values for sanity check \n",
    "# i = 0\n",
    "# print(pred_cls_cnt)\n",
    "# print(' pred_cpb ')\n",
    "# print(pred_cpb[i])\n",
    "# print(' pred_srtd ')\n",
    "# print(pred_srtd[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare output tensor from Ground Truth data \n",
    "\n",
    "Using ground truth inputs to the network, build tensor [ num-classes, num_rois, (roi_information) ]\n",
    "This Tensor will be used for training purposed on the new head we will be attaching to the existing network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gtcls_idx = mm.input_names.index('input_gt_class_ids')\n",
    "# gtbox_idx = mm.input_names.index('input_gt_boxes')\n",
    "# gtmsk_idx = mm.input_names.index('input_gt_masks')\n",
    "# print('gtcls_idx: ',gtcls_idx, 'gtbox_idx :', gtbox_idx)\n",
    "# gt_classes = sample_x[gtcls_idx][0,:]\n",
    "# gt_bboxes  = sample_x[gtbox_idx][0,:,:]\n",
    "\n",
    "# gt_cpb     = np.zeros((num_classes, num_max_gt, num_cols ))      # 4, 32, 7\n",
    "# gt_cls_cnt = np.zeros((num_classes), dtype='int16')\n",
    "# # gt_masks   = sample_x[gtmsk_idx][0,:,:,nz_idx]\n",
    "# # gt_indexes = np.arange(gt_classes.shape[0],dtype='int16')\n",
    "# # gt_probs   = np.ones(gt_classes.shape[0])\n",
    "\n",
    "# print('gt_classes.shape :',gt_classes.shape, 'gt_boxes.shape :',gt_bboxes.shape,'gt_masks.shape :', gt_masks.shape)\n",
    " \n",
    "# for i in range(num_classes) :\n",
    "#     print('indexes for class',i )\n",
    "#     class_idxs = np.argwhere(gt_classes == i )\n",
    "#     gt_cls_cnt[i] = class_idxs.shape[0]\n",
    "#     for j , c_idx in enumerate(class_idxs):\n",
    "#         gt_cpb[i, j,  0]  = j\n",
    "#         gt_cpb[i, j,  1]  = i                                   # class_id\n",
    "#         gt_cpb[i, j,  2]  = 1.0                                 # probability\n",
    "#         gt_cpb[i, j, 3:7] = gt_bboxes[c_idx,:]                         # roi coordinates\n",
    "#         gt_cpb[i, j,  7]  = c_idx                               # index from mrcnn_class array (temp for verification)\n",
    "\n",
    "# gt_cpb_all = np.empty((0,8))\n",
    "# for i in range(1,num_classes):\n",
    "#     if gt_cls_cnt[i] > 0:\n",
    "#         gt_cpb_all = np.vstack((gt_cpb_all, gt_cpb[i,0:gt_cls_cnt[i]] ))\n",
    "# print(gt_cpb_all)\n",
    "\n",
    "# print('\\n gt_cpb : (idx, class, prob, y1, x1, y2, x2)', gt_cpb.shape, '\\n')\n",
    "# print(gt_cls_cnt)\n",
    "# print(gt_cpb[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = np.apply_along_axis(lambda x: x.pdf, 0, rv)\n",
    "\n",
    "rv = list( map(multivariate_normal, mns, pp))\n",
    "print(type(rv),len(rv))\n",
    "pdf = list(map(lambda x: x.pdf, rv,  ))\n",
    "# z =  fn.pdf(pos[img,cls])\n",
    "# np.sum(pc_tensor.pred_cls_cnt,axis=1)    sum class counts across images\n",
    "# pc_tensor.pred_cls_cnt[1,0] = 9  # manipulate the class counts for class 0 - just to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import  multivariate_normal\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, width=100)\n",
    "\n",
    "img_h, img_w = config.IMAGE_SHAPE[:2]\n",
    "num_images   = config.BATCH_SIZE\n",
    "num_classes  = config.NUM_CLASSES  \n",
    "num_rois     = config.TRAIN_ROIS_PER_IMAGE\n",
    "#   print(bbox.shape)\n",
    "\n",
    "X = np.arange(0, img_w, 1)\n",
    "Y = np.arange(0, img_h, 1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "pos = np.empty((num_rois,) + X.shape + (2,))   # concatinate shape of x to make ( x.rows, x.cols, 2)\n",
    "print(pos.shape)\n",
    "pos[:,:,:,0] = X;\n",
    "pos[:,:,:,1] = Y;\n",
    "\n",
    "pp1 = np.full((32), 12.0)\n",
    "pp2 = np.full((32), 19.0)\n",
    "cov  = np.stack((pp1,pp2),axis=-1)\n",
    "\n",
    "del pp1,pp2\n",
    "print(cov.shape, prt[0].shape, prt[1].shape)\n",
    "\n",
    "prt = pc_tensor.pred_stacked\n",
    "Zout  = np.zeros((num_images, num_classes, img_w, img_h))\n",
    "\n",
    "for img in range(num_images):\n",
    "    ps     = prt[img].eval(session=k_sess)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        cls_idxs = np.argwhere(_ps[:,6] == cls).squeeze() \n",
    "#         ps = _ps[cls_idxs,:]        \n",
    "        print('cl;s:',cls,' ',cls_idxs)\n",
    "        width  = ps[:,5] - ps[:,3]\n",
    "        height = ps[:,4] - ps[:,2]\n",
    "        cx     = ps[:,3] + ( width  / 2.0)\n",
    "        cy     = ps[:,2] + ( height / 2.0)\n",
    "        means  = np.stack((cx,cy),axis = -1)\n",
    "\n",
    "        print(ps.shape, type(ps),width.shape, height.shape, cx.shape, cy.shape, type(means),means.shape)\n",
    "    \n",
    "        rv  = list( map(multivariate_normal, means, cov))\n",
    "        pdf = list( map(lambda x,y: x.pdf(y) , rv, pos))\n",
    "        pdf_arr = np.asarray(pdf)\n",
    "        print(pdf_arr.shape)\n",
    "        pdf_sum = np.sum(pdf_arr[[cls_idxs]],axis=0)\n",
    "        Zout[img,cls] += pdf_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# p = tf.Variable(pc_tensor.pred_tensor)\n",
    "# q = tf.concat(pc_tensor.pred_stacked,0)\n",
    "# init=tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     print(p.eval())\n",
    "#     rec=sess.run(recall)\n",
    "#     print(rec)\n",
    "\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "# with sess.as_default():\n",
    "#     sess.run(init)\n",
    "#     # a = tf.constant(pc_tensor.pred_tensor)\n",
    "#     print(type(p))\n",
    "#     #  tf.assign(p, a)\n",
    "#     print(p[1,1,:,:].eval())\n",
    "#     print(p[:,:,:,1].eval())\n",
    "#     sort_idx = tf.nn.top_k(p[:,:,:,1], k=32).indices\n",
    "#     print(sort_idx.eval())\n",
    "#     print(p.shape)\n",
    "#     p_sorted[0] = tf.gather(p[0],sort_idx[0],axis = 2)\n",
    "#     print(p_sorted.eval())\n",
    "#     print(q.shape)\n",
    "#     print(q.eval())\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training head using  Keras.model.fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=69, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training heads using train_on_batch()\n",
    "\n",
    "We need to use this method for the time being as the fit generator does not have provide EASY access to the output in Keras call backs. By training in batches, we pass a batch through the network, pick up the generated RoI detections and bounding boxes and generate our semantic / gaussian tensors ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_in_batches(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs_to_run = 2,\n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate one training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "np.set_printoptions(linewidth=100)\n",
    "learning_rate=model.config.LEARNING_RATE\n",
    "epochs_to_run = 2\n",
    "layers='heads'\n",
    "batch_size = 0\n",
    "steps_per_epoch = 0\n",
    "# assert self.mode == \"training\", \"Create model in training mode.\"\n",
    "# Pre-defined layer regular expressions\n",
    "layer_regex = {\n",
    "    # all layers but the backbone\n",
    "    \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # From a specific Resnet stage and up\n",
    "    \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # All layers\n",
    "    \"all\": \".*\",\n",
    "}\n",
    "\n",
    "if layers in layer_regex.keys():\n",
    "    layers = layer_regex[layers]\n",
    "if batch_size == 0 :\n",
    "    batch_size = model.config.BATCH_SIZE            \n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = model.config.STEPS_PER_EPOCH\n",
    "\n",
    "# Data generators\n",
    "train_generator = data_generator(dataset_train, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size)\n",
    "val_generator   = data_generator(dataset_val, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 augment=False)\n",
    "\n",
    "# Train\n",
    "log(\"Last epoch completed : {} \".format(model.epoch))\n",
    "log(\"Starting from epoch {} for {} epochs. LR={}\".format(model.epoch, epochs_to_run, learning_rate))\n",
    "log(\"Steps per epoch:    {} \".format(steps_per_epoch))\n",
    "log(\"Batchsize      :    {} \".format(batch_size))\n",
    "log(\"Checkpoint Folder:  {} \".format(model.checkpoint_path))\n",
    "epochs = model.epoch + epochs_to_run\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "if not gfile.IsDirectory(model.log_dir):\n",
    "    log('Creating checkpoint folder')\n",
    "    gfile.MakeDirs(model.log_dir)\n",
    "else:\n",
    "    log('Checkpoint folder already exists')\n",
    "\n",
    "model.set_trainable(layers)            \n",
    "model.compile(learning_rate, model.config.LEARNING_MOMENTUM)        \n",
    "\n",
    "out_labels = model.keras_model._get_deduped_metrics_names()\n",
    "callback_metrics = out_labels + ['val_' + n for n in out_labels]\n",
    "\n",
    "progbar = keras.callbacks.ProgbarLogger(count_mode='steps')\n",
    "progbar.set_model(model.keras_model)\n",
    "progbar.set_params({\n",
    "    'epochs': epochs,\n",
    "    'steps': steps_per_epoch,\n",
    "    'verbose': 1,\n",
    "    'do_validation': False,\n",
    "    'metrics': callback_metrics,\n",
    "})\n",
    "\n",
    "progbar.set_model(model.keras_model) \n",
    "\n",
    "chkpoint = keras.callbacks.ModelCheckpoint(model.checkpoint_path, \n",
    "                                           monitor='loss', verbose=1, save_best_only = True, save_weights_only=True)\n",
    "chkpoint.set_model(model.keras_model)\n",
    "\n",
    "progbar.on_train_begin()\n",
    "epoch_idx = model.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch_idx >= epochs:\n",
    "    print('Final epoch {} has already completed - Training will not proceed'.format(epochs))\n",
    "\n",
    "# while epoch_idx < epochs :\n",
    "progbar.on_epoch_begin(epoch_idx)\n",
    "steps_index = 0\n",
    "# for steps_index in range(steps_per_epoch):\n",
    "\n",
    "batch_logs = {}\n",
    "print(' self.epoch {}   epochs {}  step {} '.format(model.epoch, epochs, steps_index))\n",
    "batch_logs['batch'] = steps_index\n",
    "batch_logs['size']  = batch_size\n",
    "progbar.on_batch_begin(steps_index, batch_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgmeta_idx= model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta  =  train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outs = model.keras_model.train_on_batch(train_batch_x, train_batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(outs, list):\n",
    "    outs = [outs]\n",
    "for l, o in zip(out_labels, outs):\n",
    "    batch_logs[l] = o\n",
    "\n",
    "    progbar.on_batch_end(steps_index, batch_logs)\n",
    "\n",
    "        # print(outs)\n",
    "    progbar.on_epoch_end(epoch_idx, {})\n",
    "    # if (epoch_idx % 10) == 0:\n",
    "    chkpoint.on_epoch_end(epoch_idx  , batch_logs)\n",
    "    epoch_idx += 1\n",
    "\n",
    "# if epoch_idx != self.epoch:\n",
    "# chkpoint.on_epoch_end(epoch_idx -1, batch_logs)\n",
    "model.epoch = max(epoch_idx - 1, epochs)\n",
    "\n",
    "print('Final : self.epoch {}   epochs {}'.format(model.epoch, epochs))\n",
    "# end if (else)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [TF_gpu]",
   "language": "python",
   "name": "Python [TF_gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
