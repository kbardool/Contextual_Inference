{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Test on Shapes Dataset\n",
    "\n",
    "Run the Mask R-CNN net in inference mode, with the additional PCILayer that generates the context-based tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\envs\\TF_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'mrcnn.pc_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8857d001f061>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpc_prototype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmrcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpc_layer\u001b[0m    \u001b[1;32mimport\u001b[0m \u001b[0mPCNLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPCILayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Root directory of the project\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'mrcnn.pc_layer'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pprint\n",
    "import keras.backend as KB\n",
    "sys.path.append('../')\n",
    "\n",
    "import mrcnn.model     as modellib\n",
    "import mrcnn.visualize as visualize\n",
    "import mrcnn.shapes    as shapes\n",
    "from mrcnn.shapes      import ShapesConfig\n",
    "from mrcnn.config      import Config\n",
    "from mrcnn.model       import log\n",
    "from mrcnn.dataset     import Dataset \n",
    "# from mrcnn.pc_prototype import PCTensor\n",
    "from mrcnn.pcn_layer    import PCNLayer, PCILayer\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "MODEL_PATH = 'E:\\Models'\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(MODEL_PATH, \"mrcnn_logs\")\n",
    "# Path to COCO trained weights\n",
    "COCO_MODEL_PATH   = os.path.join(MODEL_PATH, \"mask_rcnn_coco.h5\")\n",
    "RESNET_MODEL_PATH = os.path.join(MODEL_PATH, \"resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "\n",
    "print(\"Tensorflow Version: {}   Keras Version : {} \".format(tf.__version__,keras.__version__))\n",
    "pp = pprint.PrettyPrinter(indent=2, width=100)\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "# Build configuration object -----------------------------------------------\n",
    "config = shapes.ShapesConfig()\n",
    "config.BATCH_SIZE      = 2                    #Batch size is 2 (# GPUs * images/GPU).\n",
    "config.IMAGES_PER_GPU  = 2\n",
    "config.STEPS_PER_EPOCH = 7\n",
    "config.IMAGES_PER_GPU  = 1\n",
    "config.display() \n",
    "\n",
    "# Build shape dataset        -----------------------------------------------\n",
    "\n",
    "# from mrcnn.datagen import data_generator, load_image_gt\n",
    "\n",
    "# Training dataset generate 500 shapes \n",
    "dataset_train = shapes.ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = shapes.ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()\n",
    "\n",
    "# Load and display random samples\n",
    "# image_ids = np.random.choice(dataset_train.image_ids, 3)\n",
    "# for image_id in [3]:\n",
    "#     image = dataset_train.load_image(image_id)\n",
    "#     mask, class_ids = dataset_train.load_mask(image_id)\n",
    "#     visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile_only(learning_rate=config.LEARNING_RATE, layers='heads')\n",
    "KB.set_learning_phase(1)\n",
    "print(' Learning phase values is L ' ,KB.learning_phase())\n",
    "mm = model.keras_model\n",
    "print('\\n Metrics (_get_deduped_metrics_names():) ') \n",
    "pp.pprint(mm._get_deduped_metrics_names())\n",
    "print('\\n Outputs: ') \n",
    "pp.pprint(mm.outputs)\n",
    "print('\\n Losses (model.metrics_names): ') \n",
    "pp.pprint(mm.metrics_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "inference_config.display() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_log_dir: Checkpoint path set to : E:\\Models\\mrcnn_logs\\shapes20180329T1212\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      " IMAGE SHAPE is : 128    128\n",
      "<class 'list'>\n",
      "Tensor(\"rpn_class_logits/concat:0\", shape=(?, ?, 2), dtype=float32) rpn_class_logits/concat:0\n",
      "Tensor(\"rpn_class/concat:0\", shape=(?, ?, 2), dtype=float32) rpn_class/concat:0\n",
      "Tensor(\"rpn_bbox/concat:0\", shape=(?, ?, 4), dtype=float32) rpn_bbox/concat:0\n",
      "Proposal Layer init complete. Size of anchors:  (4092, 4)\n",
      ">>> PCI Layer : initialization\n",
      ">>> PCI Layer : call\n",
      "     mrcnn_class.shape    : (?, 1000, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     mrcnn_bbox.shape     : (?, 1000, 4, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     output_rois.shape    : <unknown> <class 'tensorflow.python.framework.ops.Tensor'>\n",
      ">>> PCN Layer : call end  \n",
      ">>> MaskRCNN build complete\n",
      ">>> MaskRCNN initialization complete\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find_last info:   dir_name: E:\\Models\\mrcnn_logs\\shapes20180313T1856\n",
      "find_last info: checkpoint: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0231.h5\n",
      "Loading weights from  E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0231.h5\n",
      "load_weights: Loading weights from: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0231.h5\n",
      "load_weights: Log directory set to : E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0231.h5\n",
      "set_log_dir:  model_path (input) is : E:/Models/mrcnn_logs/shapes20180313T1856/mask_rcnn_shapes_0231.h5  \n",
      "self.epoch set to 232  (Next epoch to run)\n",
      "set_log_dir: Checkpoint path set to : E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_{epoch:04d}.h5\n"
     ]
    }
   ],
   "source": [
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:   15.00000  max:  239.00000\n",
      "image_meta               shape: (12,)                 min:    0.00000  max:  128.00000\n",
      "gt_class_id              shape: (2, 4)                min:   38.00000  max:  128.00000\n",
      "gt_bbox                  shape: (2, 4)                min:   38.00000  max:  128.00000\n",
      "gt_mask                  shape: (128, 128, 2)         min:    0.00000  max:    1.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHVCAYAAABfWZoAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8VPW9//H3mS0bCSQhEPYdI1qBgqIWlyqiRa2tS9ViC1V+P221/rC9tvXK7WLp9Vprtdu13gvWDRWtW7VWEbEtEZCwKFZANoGwJpBA9sks5/dHMjGRBLLN98yZvJ6PRx42s+UzdXnx/c7JOZZt2wIAAGZ4nB4AAICehPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBPqcHcELuvALb6RkAAPFxeP5my+kZjocVLwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQT6nBwDQMZnBiFY/EFGVJ0f/8l6ob935itMjAegAwgtXWrR4T5v3LZycrWWjMiRJF2yv1k1rytt87MxrBzf9718sKdHw8vpWH/fOyAwtOD1bkjSirF7z3ypp8zXnXdRPn+QEJElzisr1xR3VrT5uZ3ZAd0/v1/R9e95TZjCiJX/arV52qvpE9is/slWLng1LluXa9yR1z9+n5o8BEhnhBQzzRWxZklLCUfWpjTTd7o/YbT6nVzCqARUh/enFffok26+ssr6yZCvb3q9TSsPakhtoemzz10wJR9t83bRQVOn1UdUE+MQJMMmy7bb/ZU9WufMKet6bTiK/WNKwMmu+snIF29bK+3I1MlKkaCf/zFvsPUUfeS9sWuF67aBOD72sPvaBTr2eRxGt812mL/1wY6eenwhiq2pWvIg5PH+zdeJHOYcVL1ynra3ThGbbuvvvh5QfLdfSwLdVb6V3y8tGrBStClzb6ef3jh7QmaHndenmPvprQWa3zGTaOyMznB4B6BDCC3SjflVhpYajx9w+a/1RXbStSiv8N3dbdLvDUU++Vvmv0a/eeEqpYVtFg1OPfUyKV0fTvA5M1z6xz6kBtyC8QDdZdt8wjQv/XfVKO+a+Gqu31vpnJlR0Y4568rWp/gb96i9/ldeuOeZ+n+q12n+lvvKDNQ5MByQfwgt0g9lrj2hMeL/+EZitGst9K7Cjnnz9PXBTq/flRXfojNBLOn1PXxUNPvYPFU4bUdbw0UPsqGsg0RFeoBOGHqlX77qGLeUv7KrRLavLtSJwoyujeyKlnpFa579UTz7/ou6Y0V97evslSfszfTqU4fx/QmK/BsXBVXAL5/+tAVxmyX1j9bnwW6q1siRJIaXqfX9yRjem1DNSCl2l373wT3lUK0lKsau1yn+NrvrBSoenA9yF8MJ1nDyK9ap/VeiU8Dt61/91VXryHJvDCaWeESoNjGj6fkBks84MPa9TD/TVv/KPPSgLQOsIL1zHqaNYr/yoQve8XaKV/lk9Lrqt2e8tkGTpuWdf1TXXD9ZH/Ykv0B6csgZohyX3jdXvXq7QpiDRbW6/9yTtDl6mJQsO6YVfnu30OIArsOKF65g+ivXKjyp0SniZVvqvJbqtaFj5SmeGntMpB/uy8gVOgBUvXGf+WyXHPaF/d7ryowr9/K0SonsC+70F+tA3Tc8/s0enHKxzehwgobHiBdqw5L6xOiX8Dp/pttN+b4EUlJYsWKpV/q/pqh+sMPJz513ksnN2o8cjvEArYkcvr/R/jeh2gBPbzpw4A25DeAFJE/fVqqC04bPjAZVh3bS2nJVuJ8Xi+/wzr+m+c/uq3ttwoZjlw9ObTr4BMyK982XZtjwVB50eBc0QXiSdZ7/xiH701/naWVbcrsdfvKVKf3q+XCl2tYqzzlC0uk4bvVeoytM3zpMmr/3eAkWDPs1982NJkldh3R/doxX+6/S1Hyzv1p81p6hcEhdL+Kxw/7GqnfO8ZNs6edla7VnV+ilBYR7hRdK57smbW73d6/EqEo20uO3iLVV66PUDes8/U+dK2uC/RBG/Cy87mIAOekfroHd00/dDIht0duhZjT6cp225nd8errzh4Rbfn/vuv0uSHrzhP5Xy3iIFthZKkurHTFVwysw2XyfzqW83/e/qGXcpmjO01cf5txYq9b1FkqRIzlDVzLirzddMf/1eect2S5LqpsxUaMzUVh/nKdutjNfvbfM9NdeZ9xTuP1YVNz+r9MqgJOnDL50p77jHZdXXuPY9Se3/+2RJoyU9aEuXt/nCDiK8cLXJQyboZ5fcqV6BhrNZ/eTN+/XgFffo+idv0eaSrXrlpidUtHu9Jg0+TXXhoK5/8hZNP+l8/fCC25Qd8mjg0aC+U/AjXf7LAS1eN2NMX536q8sVyE2XJ+DTjt8XqviJtU68xaRR7D1NkvTSorf01ZlDOhXf4/3H3O1sy1K09wDJavnLJvUnT1O070hJUjhvpCLZbZ+Tuu6sb8r2+FQ3dbZ8nxQpkDGu6b6aIRPkKS+WbFsacYbkabjUYzQ9+7ivGZz4VXlqGnYVQsMmKdp7gKyjB2TJ7vR7NeDrTg9wPJZtJ/T/eXGRO6+g573pJBL7Pd7yQXlacftrmvX07SoqXi+P5VFmSi/9/daXWoS3sq5Ks575riLRiEblDterc57UfXdfqXt/t06rM65TReogRarq9eW6/9Jfc3+saF1Y5yz/jtbNXqyqLaXy9grovBXf1eqrn1DVllKH3737DYlsUEF4eae2nWPhbb4KWrR4jyR3XyTB9vpVdf3vlNbnNKUfqezy6/Xef1jZ+w61uK18UJ6O5ud0+bWrszPlqw9rTOEH2vbRd7v8evFQNn/za5LEihfoJrGjWC8aMkFbSrerqHi9JClqR3W0ruKYx7+w4bWmLebzR5+tf2z6h372yAq9579BR8J5UlXLreWMMX3Vq6CfJj15fdNtnhSfehX0I7zdoNh7mjyKalL4VUl9nB7HcbHoKhxUwbK18sRpMZS9t1TZe7v+z2/UY2nr1PHaOnW8tNkvKxLqhul6FsIL17Isq12Pq67/9OLuliz5oraq/ZaOhAe0+njLkuoP1+gfU37bLXPiWKWe4RodXtXh5zVf6bqJbVnqc/kLqk9NOea+yn59lFkf1ujCDXGLbnfyRG2NKfxAW6eOV/SWd9Sr9OgxjwnU1qnftr2yJG3ZdLv5IRMcZ66C68wpKteconKt3r1eY/NGafKQCZIkj+VR79Ss4z532bZCnTvufHlHNRz04wl45e3V8rPGqi2HFKmp1+CvT2y6rdfYPPkyj/2PJnAitmWp5ss/VenIga3en72n1DXRjYnFt8++w63eXzJ6sHZOLkjsT4EdxIoXrvPFHdWSpAWnH9Wsp2/X/C/9SOmBNEXtqH7yxi+P+9wdh3fp+d98X9968hmdp3zZkajWz3lOlR99+nuOdiSq1Vc+rlN/dblG33Gu5PUoWFKltTMXxfV99SRhBRRQrU49UNflSwruzE7cE2jEohvpN1qnvb1W3nDkxE9yCU/U1oDNu1q9b8DmXdp8wecb4rtJat/eVM/BwVVwna4cTNNwGkgueJAIBkQ263PhhtNLVnj66Vv/9vIJn1Pd+GsvzX9tJZHYknp/+UXVZTUcZV+Xma5wwKeTl61Lqui2R9jv0+YLPi9/Xb1SqmolSWlHq9Vv2x5tjfP2MwdXAQmi4TSQRDdRND+95Cr/19r1nLZ+hzMR2JJqL/43BUcPVt6OfZKk1Moa5e3Y1+OiK0m+UFgFy9bp0IgBshuPxzg4doiCGakmVsF3xPflu4bwoke46l8VumcpF7FPNMlyScFYdEOjztLEt9fIVx92eqSE4AuFlb/l0zPI9d25X5sunKzotLlKW/pQ3OJrS9vi9NLdgvAi6XGVocQWi2/sqkbt3XaOcfr3eG1JqV//m4L5uUT3BPzBkE5+e402XThTaX2na8gH2+K+7ZyIOKoZSY2rDLlD7Hq+Z4aeU1bUPSf0j610j+bnahzRbZdYfI8MylPx+NFxOfLZkm6zpNvi8NLdgvDCdXZmB9p1JOtV/6rQPW+XEF2X+DS+z+vUA3VOj3NCzbeXiW7HNI9v7bS58YjvxY1fCYmtZrjO3dNPfOFztpfdqWHb2dKbC9/SKv81qvD079C2c7xlXfFS02kXI36/PAEf28ud1HzbOVLwNXlDDWfA6rP/sPpv3ZPUJ94gvEg6bC+7237vSZKkM0PPa5X/mhb3+RuvaOOE2vNuVs3YoRrywVZZtiTZyjpYLl+I6HaWPxjSuKVFqujX8IcZ27JUPGG0wn6ftMnh4eKI8CLp/OztUq32X090XWy/9ySl2RUaG1nZ4vbYZd9Mqz3/FgXHf1kTlhYpUMdlI7uTrz6snD0lTd9nHjqijdMmyzpnjtKWL3BwsvjhM164zqLFe5qOZG2N17ZVa/U2OBHiocbqLUtRp8dQ+jWvyj59jib8czfRNSBQG9S4pWtkn/Vt9brqLxp7cvKdM53wAnCNSM5QRT5zEo2Fk7O1cHK2nv3GIxqeM6TDr3l4/mZlBNJbva/2vJt1aPgAjVu6hugaFItvyahB2jduuNPjdDvCi6Qya90RBb2WQuKCBm5XbWUrJ7pXp++pbbqtZsZdqmk8bWTMslEZWjYqQ9c9ebN2lhV/9mXkbbzge0fVnnezghOuILoOaR7f2qk3dfTp2xu/EhKf8SJpLLtvuMaEV2hF4EZFLf7RdrtKT57W+S/VK4//Vav9X1W5Z7CuvqHhvslDJuhnl9ypXoGGcyL/5M379eAV9+j6J2/R5pKteuWmJ1S0e70mDT5NdeGgrn/yFk0/6Xz98ILb5PP4FLWjuvWFH2njwS0tfuboviP0ixl3KW/wGbJS/Hpx/Va9SXQdE4vvxmnfUa/+V2jgxp3tOtrZluYaGK/T+K8TksIN649oTHilVgSuV42V7fQ46CalnpFa579UZ4Re0nv+qyRJ2R6fnvj67zTr6dtVVLxe03bUKjM9U7qi5XML+o3R1Y/PUSQa0ajc4XroKz/XZQtu0I7DuxTw+hXwtvxdcK/Hq0eu+ZVm7yzSx/0nafJbRXrqi5/X5v2HtKuiRnDGp/Gd3HBDEhztzFYzksJ3V5Vprf/LRDcJlXpGaqt3ioZH1kuSzkzrrS2l21VU3PD9t4oO67p/7DzmeS9seE2RaMPFCc4ffbaWbvmndhxuuIxdfSSkqvrqFo8fnTtcY/PH6rEp1+t929bCc8Yr4PFoeFavOL47tEcXt50TDiteJI16pTk9AuIkZKUpdnqj9p5Yv7r+01Wq1Y5npV/83zpseXXTy+/ymW4C6si2syW9KiXuZQFZ8cJ1YkexomdaWXtUY/NGafKQCQ03eDzyZfU57nOWbSvUtLHnamTuMElSwOtv+nxYkmqn3qhVA/oqUl2nLw/Ibbp9WFa60n2dOzgL3S9ZjnZmxQvXWTYq48QPQtIqj4Y16+nbNf9LP1J6IE1DZgb18X/9+3Gfs+PwLt3x8o+14Npfy2t5FbEjuvWFu7Sp8eCq4Ocu1UlLi3Sn36e5k07SzJOHy2NZKqur17zCD6QeeD3dRNX8M19r6k1KK1zo9EgdZtl2PK4Nkdhy5xX0vDed5N57eId2Vc5WtSfH6VEQB0MiHyo3ulun3T9AkuQt2910X1cuC5hx9asqHTFQ4zgjlevUp6Vo47TJ6rd97zHbzmXzN78mJe5WMyteuM4F2xsOimla+dq2fM6f4Ahx5lG0RXC7qva8m1VDdF3LzUc78xkvXOemNeW6aU15wze2rbv/fkjVAUu1VqazgyFuyqxByovu1KWbK7vl9T49OQbRdTO3Hu3MihfuZdtaeV+u8qPlWuG/WVHL7/REiJNqT45W+a/R719/WWfVfVODltpNlwvs6BZzxtWvqmbEQC54kCSaH+2c0f8KDdq4U6ucHuoEWPHCtb73bpnyo9u0wn+d6q3Wz7WL5HHUk69b712or737v8qLftKp16g9/xaVjhjASjfJxOJbOmqQDowdIkl/aPxKSIQXrvWlLVXa4JtOdHuQj0eP06Kvzla/aMdPw1t7/i0KnnY5515OUoHaoIat/Vjlg/NkS2/Y0htOz9QWtprhalH+7NjjhH0tP1L4xZKGa7nePb1fm89he7lnsKLu+IUVwgvA1YaXHz+kdVNmcvRyD2NJl0hSoq56CS+ApBYadaaGf7CV6PYstzb+lfAC3SF2FOtbj+5yeBK4heWOHUj0EHxABsA18nccVtah6hM/sJEtyU7NUtMVFoAEwIoXgGvMufM1jQx/1K7H2pJqL/432am9lHXwYHwHAzqA8MJ1YkexAm2xJaV+/W8K5udq4ttr5AuFnR4JaEJ44TonOooVPcs7I1terSq20g3m52rc22vkqye6SCx8xgvANea/MEtLbjyjxW0LTs/WgtMbrs8ci25o1FlEFwmLFS+ApHDM9jLR7bES9XKAMYQXgKuNKGv46GHTJd9kexmuQHgBuNr8txoOtvvKDSOUv2Mf0UXC4zNeAEBSsaSHLOkhp+doC+GF67wzMkMrhqapX3VYUXmdHgeGhb0+ZdhHZNnNTophSdGcYS1vQ082qvErIRFeuM7i07J0+cdVWjK6lyqstq9Ig+T0+gVXKGDX6f6/HWwIrSVF+p8kOy1LeTv2OT0ecEJ8xgtXyQxGtO6BGh21Tlbf0osky3J6JBg0448rJUkr/dfoivef1/kb0rX1nCmy/X5NXFUubzji8ITAiRFeuMqdyw9LStMO72Si2wN9/q0tkqSIlaJV/mt0judRlZfbqvamEV24BlvNcJXHJ/ZRb/ugJodfdnoUOMm2NS78jvb2G6IjGVl8tgtXYcULV9meG9C7QwM6Y+8hvTXtcV20dJbTI8GgdReNlWVHNetvv9eBgVt0+R/u0UmaqAEf73Z6NKDdWPHCdWoCHq0elKqfLitVRrTM6XFg0Ou3nCV77CH1sQ/q2usGqyo9VTuHDtee/EFOj4bE8mbjV0JixQtXqgl4tKuPX/5DdU6PAsOyqo7qsGeQqlKqnB4FCcqWfu/0DMfDiheAq81+5gldveQVp8cA2o3wAnAl2+tXeNgknV/4T035cK3T4yCBWNJoSxrt9BxtYasZgOtEPR5VXf87WbUVSqmudXocJJ4HG/+akFcpYsUL15l3UT/Nu4gzVvVUtiWtmVGgzPSTNXGDR/w2N9yGFS9c55OcgNMjwEHVvdMUOezV6MIN8vD7u3AhwgvANeZd9bhqzjqkdLua6MK1CC9cZ05RudMjAECnEV64zhd3VDs9AhLI3n4DnB4B6BDCC8A1Ftx/mc7e8qbGfVjRdNtD3/i2gxMBHUd4AbjGgZG5Cu71Oz0GEt8dTg9wPIQXAJBUbGmb0zMcD7/HC8DV7n/gx7r/gR87PQbQboQXAJBULOk2S7rN6TnaQnjhOjuzA9qZHdDmvIBGRVbLsiNOjwRDso8c1nUvPK3dA4Y7PQoS28WNXwmJz3jhOndPbzhdZCAc1ePVexT1/1H/9ysD9b8PftXhyRBPAbtav/7pN7T4y5fqP68cp4k7nZ4I6BxWvHCtep9Hs64eqLSQrf95eZ/EmYySVsCu0dmhZ/XK5OGaP/syyeIMzXAvwgtXq/d5NPvqgZqyp1YZNme0Slb9o9tVY/XWAyP3yb97ndPjAF1CeOE6ixbv0aLFe5q+D/o8qgrwj3Kyq1caK10kBT7jBeAakZyhiqZnt7jthWkJeclVoE2EF4Br1My4S5HswfJvX9F026rxpzs4ERLUdqcHOB7CCwBIKrY01+kZjocPxpA0LEWdHgFx0uLv7Wc+5z3zgyKd+UGR4YmAziO8SApLR/XS+PCb8tr1To+CbpYRLdNJ4XdV4hkhOyVD0Zwh8m97t+n+q5a+qquWvurghEDHEF4khXkX5emv4yuV0/+Puu32F50eB90kI1qmgtQ/6Ucz0nXhr6XI0ElKPxrUqd6rnR4NCcySXrWkhP3TGOGF6yycnK2Fk1se2Wpblr43o7+25gb07LN7JJttZ7dLtSt1duhZ/fKcXD1+foEqZy9Q2pFqBWqDTo8GdAnhhessG5WhZaMyjrk9Ft+CQ0GlqNaBydCd+kT3q8KTp6cm9pFl2xInJkOSILxIKrZlKezhJAvJIiqvJMlTcVCZj92o2pSQghsWasum2x2eDOg8fp0IrnPB9mpJanXVG+NVyNQ4iJPP/j30lWxT5qOzVDn7UXkP73JoKqDrWPHCdW5aU66b1rR9Xuanx/fW5NDL8ttsN7tVVrREp4TfUbHn1Ba3+0q2KfXdPyk06kyHJgO6jhUvks7Pz+8rX6RUU3c9oqu+PkQP/uEqp0dCB2RFS1QQeEK3XtZPL4/b2OK+uikzFR4yQZ6KA0233fn9e0yPCHQJK14kH8vSTy7MU+GwdL3wdDErXxfJipbozNBzunt6P708LuuY+0NjpiqSN9KByeAyf2j8SkiEF8mpMb7vDkvXWaHF8tt1Tk+EE4hF91++C1uNLtBetvSGLb3h9BxtIbxIXpalH1+Yp0Vn1GtAzh8199YXnJ4IbWjaXv5qli7+4ccdeu7cJx/W3CcfjtNkQPcjvEhujfF9d1i6XmTbOSGdaHv5RAaV7Negkv1xmAxuZUmXWNIlTs/RFsKL5Ncsvg3bzsQ3UbC9jDi5tfErIXFUM1xn5rWDO/6kxvja4mjnRNHy6OWObS8DbsaKFz1HswOuHnthr9PT9Gg+O9ip7WVP2W5ZNW3/DjfgBoQXPYtl6Q9TcjTmEJcPdFJAtYrI1+Ht5YzX71XKxqVxmgowg/DCdX6xpES/WFLi9BgA0Cl8xgvXGV7etdVqyCv1CkWVoioFrV7dNBU6IsMul211z5/73/vcpG55HcAUVrzoccrSffrtWbk6O/SMUuwqp8fpcbKje/X50Gv6l/fCDj+38oaHVTf5mha3/Xn6Ffrz9Cu6azwg7ljxokd6YGquZB/S1R/9r664YYju++PVTo/UI2RH96rA/4y+eWW+lo7+0OlxkKRs6XKnZzgewose64Fz+kqSXnmqWJ/YbDvHW3Z0r84IvdgY3e77/3rwgYYj1PfkD+q21wTiifCiR4vF97vLn9En3kmSLNmytM9ToJCV6uxwLpcRLVOe3XDdXI8d1pjIKq33zej2le7/W/SIJK5SBPcgvOjxHjinr0p6HdH4A6slSfmVYQ2o/Lv2l91CfDspdnKMt0dlqN5nSZLmnZyj5cPZXkb8WdJDkmRLc52epTWEF67zzsiMbn/NJyf20ZOxb2xb97xdqpkHn9VK/3XEt4Nip4FsOCNV958G0opGFBkwTtFdR+SJ2t3++kgKo5we4Hg4qhmus+D0bC04PTt+P6Dx9JKHPUN1VuhZzu3cASbOvWwdPSCr9qi2Th2vqMeKy88A4onwAq2xLJ3xw6N66ox6Dch5hPi2Q1cu7ddeKe8tUup7T6nX4jtUVblB68dbYs0LtyG8cJ0RZfUaUWbglI+N53Yu5KpGJ9TVS/u1V2BroQJbC2VFQuq1+A7ZgTQFe6XF7ecB8UB44Trz3yrR/LcMnTKyMb6HPEOJbxucurSfFQnJt3u9bIvtZrgLB1cBJ2JZmvLDo/rZ2/WauusR7S+7WSGLVZZk/tJ+9WOmSmpY+cb87M67NeDj3XH/2UB3YcULtAfbzscwtb3cXHDKTAWnzGxx286hwzl5Bj7rzcavhMSKF2ivxvhesqa3vhB6WmXWYElS2AroY+8XFLECDg8YXxl2uUaGixQ7nCk/urVxe5mL2COx2NLvnZ7heAgv0BGN286Xbw4ot2a/JOms4hoNqvhQZQdvSdr4ZkTLVJD6Jz03OUt7svySpM15OVo11Pnozn7mCWXvO8SFEuAahBfoKMvSqydnNn37+Od769evH9Sle57XKv81SRffjGiZzgo9q7suytVTE/s4PU4LnooSnb1utTIPHSW8aGJJoyXJlrY5PUtrCC/QRbZl6Xsz+uu8Dak6J/SUKq3cYx5TY/XWZu85si2vAxOeWMCu1snhf8in0DH35UT36GPfVD018RMHJju+lDXPyRcarKq+vRX2++QLhZ0eCYnhwca/JuRViggvXGfeRf2cHuEYtmVp0o/qdPHWgNJCR4+5/7oNxYoGPpJnxy0JF9+AXa1hvf9HbwxPV9GgY4/W3puVrdVDEi+6kmRJihavUrT/WK0/K0cTV5YRXyQ8wgvX+SQnMbdybcvSG2Nbv9zdX0/qpcf/vE+nhl/VWt/lCRPfgF2gA7DPAAALS0lEQVSts0PP6o8nZeq+c3Mll/5OrPfgFnmqynRoxADlbyl2ehzguAgvYEC9z6PZVw/U+vtDOjf0hILq3IUeir2naK/3lE9vsG2dFFmu7OiBTr1eL/uwir2n6r5zD7kiuplPfbvN+zzlxbIDAw1OA3QO4YXrzCkql6T4XighDoI+jybcKZ29OyCPfexnqSeSForq3iVv6LFz1+iipbMk29Zp4SU6MPBj/ccXcjp1BqeqQG+tHuyO6ALJgvDCdb64o1qS+8IrNax8/96FyxpuzkvRy4uKVRL5QH2i+5VpH9J51w1WVUrPPRfOzuyGjx68pTtUeu6V6rtzv/zBjv/BBjCl5/7bCrjQ9tyAvjJziPJT39KBgVv0+e+n96joVs+4S9Uz7mpx293T++nu6f2UUrRY0Q8W6f2pA7V5148dmhA4Mcu2e95FtXLnFfS8N51EFi3e0+L7ndkB3T29X5v3N7dwcraWjWpYcV6wvVo3rSlv87Ezrx3c9L9/saREw8tbvyLSOyMzmlbfI8rqj3sBh3kX9Ws6OGxOUXnT6v2zTvSePFFbttVwQFeyvKfm2npPkcGfkyR593zY6nuyJdVOm6tQwRc1oXAfK98e5kh+rvaPG6ajA/qOkRL393h7zh+VgSQS9VhclacVlqS0pQ/Jv/kdbbpwskIpfqdHggNsaVuiRldixQvARSpveFjS8Y9ullqufDMfnaWCYfcYmA5Oi614jwzom9B/KuXgKgBJJ7bylaTKGx9XePle+eo5sUZPYUm3SYl7sQS2mgEkpVh8rZpyVfTLcXocmHVx41dCIrwAkpYlyaqrdHoMoAW2mgG4hn9rYaeex4FoSCSEF4BrpL63qMPP8W8tVPEXf6DMQ0cUqA3GYSqgY9hqBpDUUosWy1r5sN4/d5g275nv9DgAK14A7hHJGSpJ8pbt7tDz0pYvkCRV3PSE6v+5i5VvkrK97vhIgRUvANeomXGXaj5zysj2Slu+QClr/6yN0yarPi2lmyeD04Lpqdo5qUA5xSWStL3xKyERXgA9RtryBbJW/DfbzkkmmJ6qD84bLs/y3+roX66ULc21pblOz9UWtpoB9ChphQslse2cLILpqdo4bbJSl/9GqSsed3qcdmHFC6DHSStcqJQ1z7Pt7HKx6OZv2e2a6EqEF0APlVa4kG1nF4ttL3uX/0aVL32lxX2W9KolverQaCfEVjOAHottZ3dy4/Zyc6x4AfRobDu7i1u3l5tjxQvANdJfvzcur5tWuFC1kt4/9xplLfymCgbPi8vPQdfEtpdTl/9GlS6NrkR4AbhIR0+c0RFsOyc2t28vN8dWMwA0Yts5MSXD9nJzhBeAa9RNmam6KTPj+jM42jmxHO/oZbdiqxmAa4TGTJXUuasUdURa4ULJsth2dlgXtpf/EK+ZugMrXgBoBed2dlYwPaVxe7m4w9vLtvSGLb0Rp9G6jPACQBvSli/gkoIOaNheHiHP8t+q8qUrnB6n27HVDADHwSUFzYptL6cs/63SVjzWqdewpEukhpVvd87WXQgvAJxA2vIFkm1r47TvaPCG7bJsW5LU+8Bh+erDDk/nbqGAXxX5OZIk27JUPH608rfsVmUno9vo1sa/El4AcKu0woWqC1Zp98gzJUnRtCzZYwYp89FZKhh2j8PTuVMoJaD3zxkkq+KgPHWVkiTf0udUufbPDk8WX4QXgGt44ngCjfZILVosFS2WJNmSaqfNVeWNjytUuE/+YMjR2dwmlBLQxmmT5V/3mNKX/d7pcYwivABcIyNOp4zsDEtS2tKHJEmbLpypk99eQ3zbKRbdnN0HVdvDoitxVDMAdFosvtEPFun9qQO1edePnR4p4cW2l+11j6n2uUudHscRhBcAuiAWX//mdxq2nVP8To+UsJq2lz96s8dtLzfHVjMA16i84WFJUuZT33Z4kpY+u+3c95P9DbdHo8rbsU++UM888jkc8Kl0xEDZnoY1XunIgT12e7k5wgsA3SAW32D5HpX0HSFJiuQO056z+irzsTkqGPmfzg5oWDjg06YLJqn+wHvylhVLkrzLtqt23Ytx/9m2dHncf0gXEF4A6CaWpNRmvwpjS6q57D9UOXuBwivLeszKNxbdzJIjCj5zuyynB0owhBcA4sSSlP7az1Vz2X9o04WXqff+w8c8xlcfVv7Hu+SJ2uYH7IKox9L+gmGK+I/NyJGBfZV1sFzD1n2srQ7MlugILwDEUSy+9ZM2qrxX32PuDw0/XfvHW+q1+Hs6aewD5gfshKjH0pZzJ6i6aqN8u9cdc79nS6mC6150LLqW9JAk2dJch0Y4LsILAHFmSUpZ+0Kr96UWPqqqax9U1bUPKvqBnfAr31h0PZGoei26VVY0IbfPRzk9wPEQXgBwkBUJqdfiO1R17YPadOFkpR+p7PJr9t53WDl7S1vcVja4n44OyOnya1dnZypQW6/RhRu0LTGjm/AILwDXSHlvkdMjxEUsvvXjL1elP7VrL+bxquSc/6P00kX6XP10SVLpiIHacWqe0v65QLIjXZs1VCf7/b8Q3S4gvABcI7C10OkR4saKhJTSTb9q49u+UpWzH9WhDw/ItjzaPWG0sv7nq/Ie+qRbXh9dw5mrACDJ+Eq2KfOxG7Vr4ljtnjBa495eQ3QTCCteAK5RP2aq6k+7THZa71bvb35Gq+oZdymaM7TVx/m3Fiq1cds6kjNUNTPuavNnpr9+r7yNV0WqmzJToTFTW32cp2x3i4s4xM6y1ZqU9xY1rd7rx0xVcMrMNh/b2fdUO/37svetlWxp45iQNObTedz6ntr79ynRseIF4BrBKTPbjC6OZYXrZUXqnR7DCW86PcDxWLad2Ieux0PuvIKe96YBoIc4PH9zQp8sixUvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMs27adngEAgB6DFS8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAIMILAIBBhBcAAIMILwAABhFeAAAMIrwAABhEeAEAMIjwAgBgEOEFAMAgwgsAgEGEFwAAgwgvAAAGEV4AAAwivAAAGER4AQAwiPACAGAQ4QUAwCDCCwCAQYQXAACDCC8AAAYRXgAADCK8AAAYRHgBADCI8AIAYBDhBQDAoP8P8oGzaoPr4U4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f7e5cfcda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "# Validation dataset\n",
    "# dataset_val = shapes.ShapesDataset()\n",
    "# dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "# dataset_val.prepare()\n",
    "\n",
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    load_image_gt(dataset_val, inference_config, image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_bbox)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (128, 128, 3)         min:   15.00000  max:  239.00000\n",
      "molded_images            shape: (1, 128, 128, 3)      min: -105.70000  max:  135.10000\n",
      "image_metas              shape: (1, 12)               min:    0.00000  max:  128.00000\n"
     ]
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lay = mm.layers[229]\n",
    "# print(lay.__class__, lay.__class__.__name__)\n",
    "# pp.pprint(dir(lay))\n",
    "# pp.pprint(lay.input_spec.__dict__)\n",
    "# pp.pprint(lay.output.__dict__)\n",
    "# print(type(lay.output))\n",
    "# print(keras.backend.is_keras_tensor(lay))\n",
    "# print(K.eval(lay.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred_index.shape, pred_class.shape, pred_prob.shape)\n",
    "# b_cpb = np.column_stack((pred_index, pred_class, pred_prob, rois)) # , b_probs)) #.transpose()\n",
    "# print(' b_cpb shape: ',b_cpb.shape,'\\n',b_cpb)\n",
    "\n",
    "# print(b_cpb[:,3:] bbox_delta)\n",
    "\n",
    "# nonbg_idx = np.argwhere(b_cpb[:,1]) \n",
    "\n",
    "# print(type(nonbg_idx))\n",
    "# b_cpb_nonbg = b_cpb[nonbg_idx,:].squeeze()\n",
    "\n",
    "# print(b_cpb_nonbg)\n",
    "# order = b_cpb_nonbg[:,2].argsort()\n",
    "\n",
    "\n",
    "\n",
    "# print('\\n srtd_cpb : (idx, class, prob, y1, x1, y2, x2)',srtd_cpb.shape, '\\n')\n",
    "# print(srtd_cpb)\n",
    "\n",
    "# # srtd_cpb_2 has (idx, cls_idx, prob, cx ,cy, width, height) instead of (idx, cls_idx, prob, y1, x1, y2, x2)\n",
    "\n",
    "# width  = srtd_cpb[:,6]-srtd_cpb[:,4]\n",
    "# height = srtd_cpb[:,5]-srtd_cpb[:,3]\n",
    "# cx = srtd_cpb[:,4] + ( width  / 2.0)\n",
    "# cy = srtd_cpb[:,3] + ( height / 2.0)\n",
    "# print('\\n srtd_cpb_2 : (idx, class, prob, cx ,cy, width, height) instead of (y1, x1, y2, x2)')\n",
    "# srtd_cpb_2 = np.column_stack((srtd_cpb[:, 0:3], cx,cy, width, height ))\n",
    "\n",
    "# print('\\n',srtd_cpb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get next shapes from generator and display loaded shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)\n",
    "imgmeta_idx = mm.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mrcnn.callbacks import get_layer_output_1,get_layer_output_2\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "# for i in range(len(mm.outputs)): \n",
    "#     print('shape of output layer: {}  is {}'.format(i, mm.outputs[i].shape))\n",
    "\n",
    "# for i in (self.model.input):\n",
    "    # print('input  type: {}'.format(i.get_shape()))\n",
    "\n",
    "layers_out = get_layer_output_2(model.keras_model, train_batch_x, 1)\n",
    "\n",
    "# print('type of layer out is {} shape is {}'.format(type(layer_out), layer_out.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgmeta_idx = mm.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mrcnn.visualize import plot_gaussian\n",
    "Zout = layers_out[1]\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout[img,cls], title = ttl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.utils import trim_zeros\n",
    "np.set_printoptions( edgeitems=32, suppress=True)\n",
    "gt_bb = layers_out[3]\n",
    "print(gt_bb.shape)\n",
    "x0 = [ trim_zeros((gt_bb[0,i,:,:])) for i in range(4)]\n",
    "ps0 = np.concatenate( x0, axis=0 )\n",
    "\n",
    "x1 = [ trim_zeros((gt_bb[1,i,:,:])) for i in range(4)]\n",
    "ps1 = np.concatenate( x1, axis=0 )\n",
    "# print(np.concatenate( x1, axis=0 ))\n",
    "print(ps0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ps0.shape)\n",
    "width  = ps0[:,5] - ps0[:,3]\n",
    "height = ps0[:,4] - ps0[:,2]\n",
    "cx     = ps0[:,3] + ( width  / 2.0)\n",
    "cy     = ps0[:,2] + ( height / 2.0)\n",
    "means0  = np.stack((cx,cy,width, height),axis = -1)\n",
    "print(means0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display ground truth bboxes using load_image_gt\n",
    "\n",
    "Here we are displaying the ground truth bounding boxes as provided by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "p_original_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "# print(p_gt_class_id.shape, p_gt_bbox.shape, p_gt_mask.shape)\n",
    "print(p_gt_bbox)\n",
    "visualize.draw_boxes(p_original_image, p_gt_bbox)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "p_original_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "# print(p_gt_class_id.shape, p_gt_bbox.shape, p_gt_mask.shape)\n",
    "print(p_gt_bbox)\n",
    "visualize.draw_boxes(p_original_image, p_gt_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display bboxes from Ground Truth Info Passed to Net\n",
    "\n",
    "Display the Ground Truth bounding boxes from the tensor we've constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(linewidth=120, precision=5)\n",
    "# gt_bboxes = layers_out[5]\n",
    "# print(layers_out[5].shape)\n",
    "# print(layers_out[6])\n",
    "# print(layers_out[5][1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "p_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "gt_bboxes = layers_out[5]\n",
    "print(gt_bboxes[0,1,0:1,2:6])\n",
    "print(gt_bboxes[0,2,0:2,2:6])\n",
    "gt_bb = np.vstack((gt_bboxes[0,1,0:1,2:6],gt_bboxes[0,2,0:2,2:6]))\n",
    "gt_bb.shape\n",
    "visualize.draw_boxes(p_image, gt_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display RoI proposals generated\n",
    "\n",
    "Display bounding boxes from tensor of proposals produced by the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 1\n",
    "cls = 3\n",
    "caps = [str(x) for x in pc_tensor.pred_tensor[img,cls,:,0].astype('int16').tolist() ]\n",
    "print(caps)\n",
    "# print(pc_tensor.pred_tensor[1,3,:])\n",
    "# print(pc_tensor.pred_tensor[1,3,:,2:6])\n",
    "visualize.draw_boxes(image, pc_tensor.pred_tensor[img,cls,:,2:6], captions = caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class:\n",
    "- determine the center of each bounding box.\n",
    "- center a 2d gaussian distribution with the mean = center of bounding box and sigma = height/width\n",
    "- place dist on mesh grid\n",
    "- normalize\n",
    "- draw heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "from mrcnn.pc_layer import PCTensor\n",
    "pc_tensor = PCTensor(model)\n",
    "pc_tensor.build_predictions(sample_x)\n",
    "print(pc_tensor.pred_stacked)    # list of tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zout2 = pc_tensor.build_gaussian_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.visualize import plot_gaussian\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout1[img,cls], title = ttl)\n",
    "#     print(Zout.shape)\n",
    "# plot_gaussian(Zout)\n",
    "# Zout = pc_tensor.build_gaussian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.visualize import plot_gaussian\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout[img,cls], title = ttl)\n",
    "#     print(Zout.shape)\n",
    "# plot_gaussian(Zout)\n",
    "# Zout = pc_tensor.build_gaussian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img = 0\n",
    "# cls = 0\n",
    "# _cnt = pc_tensor.pred_cls_cnt[img,cls]\n",
    "# print(_cnt)\n",
    "# for box in range(_cnt):\n",
    "\n",
    "#     mns = means[img,cls, 0 : _cnt]\n",
    "#     print('img: ',img, 'class: ', cls, 'class count: ',_cnt, 'shape of mns :',mns.shape)\n",
    "#     # print('** bbox is : ' ,self.pred_tensor[img,cls,box])\n",
    "#     # print('    center is ({:4f},{:4f})  width is {:4f} height is {:4f} '\\\n",
    "#         # .format(mns[0],mns[1],width[img,cls,box],height[img,cls,box]))            \n",
    "#     # fn = lambda x: multivariate_normal(x, [[12,0.0] , [0.0,19]])\n",
    "#     # rv = tf.map_fn(fn, \n",
    "#     rv = np.apply_along_axis(multivariate_normal, 1, mns, [[12,0.0] , [0.0,19]])\n",
    "#     print('rv :',rv.shape, rv)\n",
    "#     _zo = rv.pdf(pos[img,cls])\n",
    "#     print('zo :',_zo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_tensor.pred_stacked[0].eval(session=k_sess)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tf.contrib.distributions\n",
    "k_sess = KB.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp1 = tf.fill([1,1,32], 12.0)\n",
    "# pp2 = tf.fill([1,1,32], 19.0)\n",
    "# pp  = tf.cast(tf.stack((pp1,pp2),axis=-1), dtype=tf.float64)\n",
    "# tf.cast([12.0, 19.00], dtype=tf.float64)\n",
    "# pp1.eval(session = k_sess)\n",
    "\n",
    "# mvn = tfd.MultivariateNormalDiag(means[0,0,0,:],scale_diag=p1)\n",
    "# mvn = tfd.MultivariateNormalDiag(means[0,0,0,:],scale_diag=p1)\n",
    "\n",
    "# with k_sess.as_default():\n",
    "#     print(mvn.mean())\n",
    "#     print(mvn.batch_shape)\n",
    "#     print(mvn.event_shape)\n",
    "#     print(pos[0,0,:,0,0,:].shape)\n",
    "#     rr = mvn.prob(pos[0,0,:,0,0,:])\n",
    "#     print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# from mrcnn.visualize import plot_gaussian\n",
    "# for i in range(0,config.IMAGES_PER_GPU):\n",
    "#     for j in range(0,config.NUM_CLASSES):\n",
    "#         ttl = 'image : {} class: {}'.format(i,j)\n",
    "#         plot_gaussian(Zout[i,j] , title = ttl )\n",
    "# # plot_gaussian(Zout[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zout = np.zeros((num_classes, 128,128))\n",
    "for i in range(1,config.NUM_CLASSES):\n",
    "    print('class: ',i)\n",
    "    for j in range(gt_cls_cnt[i]):\n",
    "        Zout[i] = bbox_gaussian(gt_cpb[i,j], Zout[i])\n",
    "print(Zout.shape)\n",
    " \n",
    "# plot_gaussian(Zout[1])\n",
    "# plot_gaussian(Zout[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "Fine tune all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=211,\n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as KB\n",
    "# if 'tensorflow' == KB.backend():\n",
    "#     import tensorflow as tf\n",
    "#     from keras.backend.tensorflow_backend import set_session\n",
    "#     # tfconfig = tf.ConfigProto(\n",
    "#         # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5),\n",
    "#         # device_count = {'GPU': 1}\n",
    "#     # )    \n",
    "#     tfconfig = tf.ConfigProto()\n",
    "#     tfconfig.gpu_options.allow_growth=True\n",
    "#     tfconfig.gpu_options.visible_device_list = \"0\"\n",
    "#     tfconfig.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "#     tf_sess = tf.Session(config=tfconfig)\n",
    "#     set_session(tf_sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training head using  Keras.model.fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=69, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training heads using train_on_batch()\n",
    "\n",
    "We need to use this method for the time being as the fit generator does not have provide EASY access to the output in Keras call backs. By training in batches, we pass a batch through the network, pick up the generated RoI detections and bounding boxes and generate our semantic / gaussian tensors ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_in_batches(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs_to_run = 2,\n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate one training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "np.set_printoptions(linewidth=100)\n",
    "learning_rate=model.config.LEARNING_RATE\n",
    "epochs_to_run = 2\n",
    "layers='heads'\n",
    "batch_size = 0\n",
    "steps_per_epoch = 0\n",
    "# assert self.mode == \"training\", \"Create model in training mode.\"\n",
    "# Pre-defined layer regular expressions\n",
    "layer_regex = {\n",
    "    # all layers but the backbone\n",
    "    \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # From a specific Resnet stage and up\n",
    "    \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # All layers\n",
    "    \"all\": \".*\",\n",
    "}\n",
    "\n",
    "if layers in layer_regex.keys():\n",
    "    layers = layer_regex[layers]\n",
    "if batch_size == 0 :\n",
    "    batch_size = model.config.BATCH_SIZE            \n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = model.config.STEPS_PER_EPOCH\n",
    "\n",
    "# Data generators\n",
    "train_generator = data_generator(dataset_train, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size)\n",
    "val_generator   = data_generator(dataset_val, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 augment=False)\n",
    "\n",
    "# Train\n",
    "log(\"Last epoch completed : {} \".format(model.epoch))\n",
    "log(\"Starting from epoch {} for {} epochs. LR={}\".format(model.epoch, epochs_to_run, learning_rate))\n",
    "log(\"Steps per epoch:    {} \".format(steps_per_epoch))\n",
    "log(\"Batchsize      :    {} \".format(batch_size))\n",
    "log(\"Checkpoint Folder:  {} \".format(model.checkpoint_path))\n",
    "epochs = model.epoch + epochs_to_run\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "if not gfile.IsDirectory(model.log_dir):\n",
    "    log('Creating checkpoint folder')\n",
    "    gfile.MakeDirs(model.log_dir)\n",
    "else:\n",
    "    log('Checkpoint folder already exists')\n",
    "\n",
    "model.set_trainable(layers)            \n",
    "model.compile(learning_rate, model.config.LEARNING_MOMENTUM)        \n",
    "\n",
    "out_labels = model.keras_model._get_deduped_metrics_names()\n",
    "callback_metrics = out_labels + ['val_' + n for n in out_labels]\n",
    "\n",
    "progbar = keras.callbacks.ProgbarLogger(count_mode='steps')\n",
    "progbar.set_model(model.keras_model)\n",
    "progbar.set_params({\n",
    "    'epochs': epochs,\n",
    "    'steps': steps_per_epoch,\n",
    "    'verbose': 1,\n",
    "    'do_validation': False,\n",
    "    'metrics': callback_metrics,\n",
    "})\n",
    "\n",
    "progbar.set_model(model.keras_model) \n",
    "\n",
    "chkpoint = keras.callbacks.ModelCheckpoint(model.checkpoint_path, \n",
    "                                           monitor='loss', verbose=1, save_best_only = True, save_weights_only=True)\n",
    "chkpoint.set_model(model.keras_model)\n",
    "\n",
    "progbar.on_train_begin()\n",
    "epoch_idx = model.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch_idx >= epochs:\n",
    "    print('Final epoch {} has already completed - Training will not proceed'.format(epochs))\n",
    "\n",
    "# while epoch_idx < epochs :\n",
    "progbar.on_epoch_begin(epoch_idx)\n",
    "steps_index = 0\n",
    "# for steps_index in range(steps_per_epoch):\n",
    "\n",
    "batch_logs = {}\n",
    "print(' self.epoch {}   epochs {}  step {} '.format(model.epoch, epochs, steps_index))\n",
    "batch_logs['batch'] = steps_index\n",
    "batch_logs['size']  = batch_size\n",
    "progbar.on_batch_begin(steps_index, batch_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgmeta_idx= model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta  =  train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outs = model.keras_model.train_on_batch(train_batch_x, train_batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(outs, list):\n",
    "    outs = [outs]\n",
    "for l, o in zip(out_labels, outs):\n",
    "    batch_logs[l] = o\n",
    "\n",
    "    progbar.on_batch_end(steps_index, batch_logs)\n",
    "\n",
    "        # print(outs)\n",
    "    progbar.on_epoch_end(epoch_idx, {})\n",
    "    # if (epoch_idx % 10) == 0:\n",
    "    chkpoint.on_epoch_end(epoch_idx  , batch_logs)\n",
    "    epoch_idx += 1\n",
    "\n",
    "# if epoch_idx != self.epoch:\n",
    "# chkpoint.on_epoch_end(epoch_idx -1, batch_logs)\n",
    "model.epoch = max(epoch_idx - 1, epochs)\n",
    "\n",
    "print('Final : self.epoch {}   epochs {}'.format(model.epoch, epochs))\n",
    "# end if (else)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [TF_gpu]",
   "language": "python",
   "name": "Python [TF_gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
